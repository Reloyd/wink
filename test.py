# test.py ‚Äî –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–µ—Ä–æ–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
import os
import re
import json
import pickle
import numpy as np
import torch
import pdfplumber
from docx import Document
from transformers import AutoTokenizer, AutoModel
from normalize import normalize_headings
from embeddings import scene_vector

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô REGEX –î–õ–Ø –†–ê–ó–ë–ò–í–ö–ò –ù–ê –°–¶–ï–ù–´ =====
COMPREHENSIVE_SPLIT = re.compile(
    r'(?=^\s*'
    r'(?:'
    # –ë–õ–û–ö 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (—Å –¥–µ—Ñ–∏—Å–æ–º)
    r'(?:\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
    r'(?:\d{1,2}-[–ïE]\.?)?\s*'
    r'(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?|–ò/–ù|I/E)\s+'
    r'[^\n]{2,140}?'
    r'\s*[-‚Äì‚Äî]\s*'
    r'(?:–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|EVENING|MORNING|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)(?:\s+\d+)?'
    r'|'
    # –ë–õ–û–ö 2: –ë–ï–ó –¥–µ—Ñ–∏—Å–∞ (—Ç–æ—á–∫–∞ –∏–ª–∏ –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –≤—Ä–µ–º–µ–Ω–µ–º)
    r'(?:\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
    r'(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'  # –±—ã–ª–æ \s+, —Å—Ç–∞–ª–æ \s*
    r'[^\n]{2,200}?'
    r'\.?\s+'
    r'(?:–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)(?:\s+\d+)?'
    r'\.?\s*$'
    r'|'
    # –ë–õ–û–ö 3: –°–ª–∏—Ç–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ "–õ–ï–°.–û–ü–£–®–ö–ê –ù–û–ß–¨"
    r'(?:\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
    r'(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'  # –±—ã–ª–æ \s+, —Å—Ç–∞–ª–æ \s*
    r'[A-Z–ê-–Ø–Å]+\.[A-Z–ê-–Ø–Å][^\n]{1,100}?'
    r'\s+(?:–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)(?:\s+\d+)?'
    r'|'
    # –ë–õ–û–ö 4: –ù–æ–º–µ—Ä.—Ç–∏–ø –õ–û–ö–ê–¶–ò–Ø - –í–†–ï–ú–Ø
    r'\d+\.\s*(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s+[^\n]{2,120}\s*[-‚Äì‚Äî]\s*'
    r'(?:–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)(?:\s+\d+)?'
    r'|'
    # –ë–õ–û–ö 5: –°–ª—É–∂–µ–±–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    r'(?:–°–¶–ï–ù–ê|–°–ï–†–ò–Ø|–¢–ò–¢–†–´)\s*\d*'
    r'|'
    # –ë–õ–û–ö 6: –§–æ—Ä–º–∞—Ç "–Ω–æ–º–µ—Ä. —Ç–∏–ø. –õ–û–ö–ê–¶–ò–Ø. –í–†–ï–ú–Ø" (—Å —Ç–æ—á–∫–∞–º–∏ –≤–º–µ—Å—Ç–æ –¥–µ—Ñ–∏—Å–æ–≤)
    r'\d+\.\s*(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'  # –±—ã–ª–æ \s+, —Å—Ç–∞–ª–æ \s*
    r'[^\n]{2,200}?\.\s*'
    r'(?:–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)\b'
    r'|'
    # –ë–õ–û–ö 7: –§–æ—Ä–º–∞—Ç "–Ω–æ–º–µ—Ä. —Ç–∏–ø. –õ–û–ö–ê–¶–ò–Ø. –ü–û–î–õ–û–ö–ê–¶–ò–Ø" (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏, –¥–ª—è —Å—Ü–µ–Ω 12, 13)
    r'(?:\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
    r'(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'
    r'[A-Z–ê-–Ø–Å][^\n.]{2,100}?\.[A-Z–ê-–Ø–Å][^\n]{2,100}?'
    r'(?:\s*/\s*[A-Z–ê-–Ø–Å][^\n]{2,100}?)?'
    r'|'
    # –ë–õ–û–ö 8: –°–ª–∏—Ç–Ω–æ–µ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ "–Ω–æ–º–µ—Ä. —Ç–∏–ø.–õ–û–ö–ê–¶–ò–Ø" –∏–ª–∏ —Å / (–¥–ª—è 10, 15)
    r'(?:\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
    r'(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'
    r'[A-Z–ê-–Ø–Å][^\n\s]{4,120}?(?:\s*/\s*[A-Z–ê-–Ø–Å][^\n]{2,100}?)?'
    r'(?=\s*(?:–î–ï–ù–¨|–ù–û–ß–¨|\n))'
    r'|'
    # –ë–õ–û–ö 9: "–Ω–æ–º–µ—Ä. –ù–ê–¢.–£ –¶–ò–†–ö–ê. –î–ï–ù–¨" (—Ç–æ—á–∫–∏ –≤–º–µ—Å—Ç–æ –¥–µ—Ñ–∏—Å–æ–≤, –¥–ª—è 3, 4, 7)
    r'\d+\.\s*(?:–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'
    r'[A-Z–ê-–Ø–Å][^\n.]{2,80}?\.[A-Z–ê-–Ø–Å][^\n]{2,80}?'
    r'(?:\s*\.\s*(?:–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT)\b)?'
    r')'
    r')',
    re.IGNORECASE | re.MULTILINE
)


def split_scenes(text: str):
    """–†–∞–∑–±–∏–≤–∫–∞ —Å –º–Ω–æ–≥–æ–ø–∞—Ç—Ç–µ—Ä–Ω–Ω—ã–º regex"""
    parts = re.split(COMPREHENSIVE_SPLIT, text)
    scenes = []
    for p in parts:
        p = p.strip()
        word_count = len(p.split())
        has_action = bool(re.search(
            r'\b(–≤—Ö–æ–¥–∏—Ç|–≤—ã—Ö–æ–¥–∏—Ç|–≥–æ–≤–æ—Ä–∏—Ç|—Å–º–æ—Ç—Ä–∏—Ç|–±–µ—Ä—ë—Ç|–∏–¥—ë—Ç|—Å–∞–¥–∏—Ç—Å—è|—Å—Ç–æ–∏—Ç|–æ—Ç–∫—Ä—ã–≤–∞–µ—Ç|–∑–∞–∫—Ä—ã–≤–∞–µ—Ç)\b',
            p, re.IGNORECASE
        ))
        if word_count >= 5 or (word_count >= 3 and has_action):
            scenes.append(p)
    return scenes

# ===== –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ –î–õ–Ø –ü–ê–†–°–ò–ù–ì–ê –ó–ê–ì–û–õ–û–í–ö–û–í =====
HEADER_PATTERNS = [
    # –ü–∞—Ç—Ç–µ—Ä–Ω 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å –¥–µ—Ñ–∏—Å–æ–º "1-2. –ò–ù–¢. –õ–û–ö–ê–¶–ò–Ø - –ù–û–ß–¨"
    re.compile(
        r'^\s*(?P<scene_no>\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
        r'(?P<period>\d{1,2}-[–ïE]\.?)?\s*'
        r'(?P<place_type>–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s+'
        r'(?P<location>[^-‚Äì‚Äî:\n]{2,140}?)\s*[-‚Äì‚Äî]\s*'
        r'(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)(?:\s+\d+)?',
        re.IGNORECASE
    ),
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 2: –ë–ï–ó –¥–µ—Ñ–∏—Å–∞ "1-2. –ò–ù–¢. –õ–û–ö–ê–¶–ò–Ø –ù–û–ß–¨"
    re.compile(
        r'^\s*(?P<scene_no>\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
        r'(?P<period>\d{1,2}-[–ïE]\.?)?\s*'
        r'(?P<place_type>–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s+'
        r'(?P<location>(?:[A-Z–ê-–Ø–Å][^\n]{0,100}?\.)?[A-Z–ê-–Ø–Å][^\n]{1,100}?)'
        r'\s+(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)(?:\s+\d+)?\.?\s*$',
        re.IGNORECASE | re.MULTILINE
    ),
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 3: –°–ª–∏—Ç–Ω–æ–µ "–õ–ï–°.–û–ü–£–®–ö–ê –ù–û–ß–¨"
    re.compile(
        r'^\s*(?P<scene_no>\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?\.?\s*'
        r'(?P<place_type>–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s+'
        r'(?P<location>[A-Z–ê-–Ø–Å][^\s]{2,40}\.[A-Z–ê-–Ø–Å][^\s]{2,80}|[A-Z–ê-–Ø–Å][^\n]{2,100}?)'
        r'\s+(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)(?:\s+\d+)?',
        re.IGNORECASE
    ),
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 4: "–Ω–æ–º–µ—Ä.—Ç–∏–ø –õ–û–ö–ê–¶–ò–Ø - –í–†–ï–ú–Ø" (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
    re.compile(
        r'^\s*(?P<scene_no>\d+)\.\s*'
        r'(?P<place_type>–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)?\s*'
        r'(?P<location>[^-‚Äì‚Äî\n]{2,120}?)\s*[-‚Äì‚Äî]\s*'
        r'(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú)\b',
        re.IGNORECASE
    ),
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 5: "–Ω–æ–º–µ—Ä. —Ç–∏–ø. –õ–û–ö–ê–¶–ò–Ø. –í–†–ï–ú–Ø" (—Å —Ç–æ—á–∫–∞–º–∏) ‚Äî –ö–õ–Æ–ß–ï–í–û–ô!
    re.compile(
        r'^\s*(?P<scene_no>\d+)\.\s*'
        r'(?P<place_type>–ò–ù–¢\.|–ù–ê–¢\.|INT\.|EXT\.)\s*'   # \s* –≤–º–µ—Å—Ç–æ \s+
        r'(?P<location>[^\n]{2,200}?)\.\s*'
        r'(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)\b',
        re.IGNORECASE
    ),
    
    re.compile(
        r'^\s*(?P<scene_no>\d+)\.\s*'
        r'(?P<place_type>–ò–ù–¢\.|–ù–ê–¢\.|INT\.|EXT\.)\s*'
        r'(?P<location>[^\n]{2,150}?)\.\s*'
        r'(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)\b',
        re.IGNORECASE
    ),

    # –ü–∞—Ç—Ç–µ—Ä–Ω 6: –¢–æ–ª—å–∫–æ –õ–û–ö–ê–¶–ò–Ø - –í–†–ï–ú–Ø (–±–µ–∑ –Ω–æ–º–µ—Ä–∞/—Ç–∏–ø–∞)
    re.compile(
        r'^\s*(?P<location>[A-Z–ê-–Ø–Å][^\n-‚Äì‚Äî]{2,100}?)\s*[-‚Äì‚Äî]\s*'
        r'(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú)\b',
        re.IGNORECASE
    ),

    # –ü–∞—Ç—Ç–µ—Ä–Ω 7: "–Ω–æ–º–µ—Ä. –ò–ù–¢. –õ–û–ö–ê–¶–ò–Ø. –ü–û–î–õ–û–ö–ê–¶–ò–Ø" (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏, —Å—Ü–µ–Ω—ã 12, 13)
    re.compile(
        r'^\s*(?P<scene_no>\d+(?:\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?)?)?\.?\s*'
        r'(?P<place_type>–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'
        r'(?P<location>[A-Z–ê-–Ø–Å][^\n.]{2,100}?\.[A-Z–ê-–Ø–Å][^\n]{2,100}?)'
        r'(?:\s*/\s*[A-Z–ê-–Ø–Å][^\n]{2,100}?)?',
        re.IGNORECASE | re.MULTILINE
    ),

    # –ü–∞—Ç—Ç–µ—Ä–Ω 8: "–Ω–æ–º–µ—Ä. –ù–ê–¢.–£ –¶–ò–†–ö–ê. –î–ï–ù–¨" (—Ç–æ—á–∫–∏ + –≤—Ä–µ–º—è, —Å—Ü–µ–Ω—ã 3, 4)
    re.compile(
        r'^\s*(?P<scene_no>\d+)\.\s*'
        r'(?P<place_type>–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?)\s*'
        r'(?P<location>[A-Z–ê-–Ø–Å][^\n.]{2,60}?\.)'
        r'[A-Z–ê-–Ø–Å][^\n]{2,80}?'
        r'(?:\s*\.\s*(?P<tod>–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|DAY|NIGHT|–†–ï–ñ–ò–ú|–†–ê–°–°–í–ï–¢)\b)?',
        re.IGNORECASE
    ),
]

def normalize_place_type(raw: str) -> str:
    if not raw:
        return ""
    low = raw.lower().replace(".", "").strip()
    mapping = {
        "–∏–Ω—Ç": "–ò–ù–¢.", "int": "INT.", "—ñ": "–ò–ù–¢.", "—ñ–Ω": "–ò–ù–¢.",
        "–Ω–∞—Ç": "–ù–ê–¢.", "ext": "EXT.", "nat": "–ù–ê–¢.",
        "–∏/–Ω": "–ò/–ù", "i/e": "I/E"
    }
    return mapping.get(low, raw.upper())

def normalize_tod(raw: str) -> str:
    if not raw:
        return ""
    low = raw.lower().strip()
    mapping = {
        "–¥–µ–Ω—å": "–î–ï–ù–¨", "day": "–î–ï–ù–¨",
        "–Ω–æ—á—å": "–ù–û–ß–¨", "night": "–ù–û–ß–¨",
        "–≤–µ—á–µ—Ä": "–í–ï–ß–ï–†", "evening": "–í–ï–ß–ï–†",
        "—É—Ç—Ä–æ": "–£–¢–†–û", "morning": "–£–¢–†–û"
    }
    return mapping.get(low, raw.upper())

def heuristic_parse(line: str):
    result = {"scene_no": "", "period": "", "place_type": "", "location": "", "tod": ""}
    
    num_match = re.search(r'\b(\d+\s*-\s*\d+(?:\s*-\s*[A-Za-z–ê-–Ø–Å])?|\d+)\b', line)
    if num_match:
        result["scene_no"] = num_match.group(1).strip()
    
    type_match = re.search(r'\b(–ò–ù–¢\.?|–ù–ê–¢\.?|INT\.?|EXT\.?|[–∏—ñ–Ω–∞—Ç]+\.?)\b', line, re.IGNORECASE)
    if type_match:
        result["place_type"] = normalize_place_type(type_match.group(1))
    
    tod_match = re.search(r'\b(–î–ï–ù–¨|–ù–û–ß–¨|–í–ï–ß–ï–†|–£–¢–†–û|–¥–µ–Ω—å|–Ω–æ—á—å|–≤–µ—á–µ—Ä|—É—Ç—Ä–æ|DAY|NIGHT)\b', line, re.IGNORECASE)
    if tod_match:
        result["tod"] = normalize_tod(tod_match.group(1))
        if result["place_type"] and result["tod"]:
            loc_pattern = rf'{re.escape(result["place_type"])}\s*(.+?)\s*[-‚Äì‚Äî:]\s*{re.escape(result["tod"])}'
            loc_match = re.search(loc_pattern, line, re.IGNORECASE)
            if loc_match:
                result["location"] = loc_match.group(1).strip().strip('.')
        elif result["tod"]:
            loc_match = re.search(r'(.+?)\s*[-‚Äì‚Äî:]\s*' + re.escape(result["tod"]), line, re.IGNORECASE)
            if loc_match:
                result["location"] = loc_match.group(1).strip().strip('.')
    
    return result

def parse_header(scene_text: str):
    lines = scene_text.splitlines()
    first_line = lines[0] if lines else scene_text[:200]
    
    for pattern in HEADER_PATTERNS:
        m = pattern.search(first_line)
        if m:
            return {
                "scene_no": (m.groupdict().get("scene_no") or "").strip(),
                "period": (m.groupdict().get("period") or "").strip(),
                "place_type": normalize_place_type(m.groupdict().get("place_type") or ""),
                "location": (m.groupdict().get("location") or "").strip().strip('. '),
                "tod": normalize_tod(m.groupdict().get("tod") or "")
            }
    
    return heuristic_parse(first_line)

# ===== IO helpers =====
CAST_LINE_RE = re.compile(r'^\s*\[.*?\]\s*$', re.MULTILINE)
UNDERLINE_MARK_RE = re.compile(r'\{\.underline\}', re.IGNORECASE)
BOLD_MARK_RE = re.compile(r'\*\*(.*?)\*\*')
LINE_BACKSLASH_RE = re.compile(r'\\\s*$')
EP_RE = re.compile(r'\[\s*ep\s*:\s*([^\]]+)\]', re.IGNORECASE)

def read_pdf(path):
    txt = ""
    with pdfplumber.open(path) as pdf:
        for p in pdf.pages:
            t = p.extract_text() or ""
            txt += t + "\n"
    return txt

def read_docx(path):
    doc = Document(path)
    parts = []
    for p in doc.paragraphs:
        t = (p.text or "")
        t = UNDERLINE_MARK_RE.sub('', t).replace('{.smallcaps}', '')
        t = BOLD_MARK_RE.sub(r'\1', t)
        t = LINE_BACKSLASH_RE.sub('', t)
        parts.append(t)
    txt = "\n".join(parts)
    txt = CAST_LINE_RE.sub('', txt)
    txt = re.sub(r'[ \t]+\n', '\n', txt)
    txt = txt.replace("\\[", "[").replace("\\]", "]")
    txt = re.sub(r"[ \t]*\\\\\s*$", "", txt, flags=re.MULTILINE)
    return txt

def read_script(path):
    if path.lower().endswith(".pdf"):
        return read_pdf(path)
    elif path.lower().endswith(".docx"):
        return read_docx(path)
    else:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read()

# ===== Rule-based keywords =====
def load_keywords(folder="keywords"):
    cats = ["violence", "sexual", "profanity", "alcohol_drugs", "scary"]
    keywords = {}
    weights = {}
    for cat in cats:
        path = os.path.join(folder, f"{cat}.txt")
        words, w = [], {}
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue
                    if ":" in line:
                        word, weight = line.split(":", 1)
                        word = word.strip()
                        weight = float(weight.strip())
                    else:
                        word, weight = line, 1.0
                    words.append(word)
                    w[word] = weight
        keywords[cat] = words
        weights[cat] = w
    return keywords, weights

keywords, keyword_weights = load_keywords()

def find_triggers_weighted(text, words, weights):
    hits = []
    low = text.lower()
    total_score = 0.0
    for w in words:
        weight = weights.get(w, 1.0)
        for m in re.finditer(rf'\b{re.escape(w)}\b', low):
            start = max(0, m.start() - 25)
            end = min(len(text), m.end() + 25)
            snippet = text[start:end].replace("\n", " ")
            hits.append({"offset": m.start(), "match": w, "weight": weight, "snippet": snippet})
            total_score += weight
    return hits, total_score

def rule_based_score(scene_text):
    text = scene_text[:8000]
    result = {k: 0.0 for k in keywords}
    episodes = {k: [] for k in keywords}
    for cat, words in keywords.items():
        if not words:
            continue
        trig, total = find_triggers_weighted(text, words, keyword_weights[cat])
        episodes[cat].extend(trig)
        score = min(1.0, np.log1p(total) * 0.25)
        result[cat] = score
    return result, episodes

# ===== Manual ep features =====
MAP_KEY = {"v": "violence", "p": "profanity", "s": "sexual", "a": "alcohol_drugs", "sc": "scary"}
SEV_TO_NUM = {"None": 0.0, "Mild": 0.33, "Moderate": 0.66, "Severe": 1.0}

def parse_ep_features(text):
    max_sev = {k: 0.0 for k in keywords}
    count = {k: 0 for k in keywords}
    for m in EP_RE.finditer(text):
        payload = m.group(1)
        fields = {}
        for part in [x.strip() for x in payload.split(",") if x.strip()]:
            if "=" in part:
                k, v = [t.strip() for t in part.split("=", 1)]
                fields[k.lower()] = v
        for short, full in MAP_KEY.items():
            if short in fields:
                sev_val = SEV_TO_NUM.get(fields[short].title(), 0.66)
                max_sev[full] = max(max_sev[full], sev_val)
                count[full] += 1
        if "cat" in fields:
            full = MAP_KEY.get(fields["cat"].lower(), fields["cat"].lower())
            sev = fields.get("sev", "Moderate").title()
            sev_val = SEV_TO_NUM.get(sev, 0.66)
            if full in max_sev:
                max_sev[full] = max(max_sev[full], sev_val)
                count[full] += 1
    cats = list(keywords.keys())
    vec = [max_sev[c] for c in cats] + [count[c] for c in cats]
    return vec

# ===== ML Model =====
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
EMB_MODEL = "ai-forever/ruRoberta-large"
tok = AutoTokenizer.from_pretrained(EMB_MODEL)
mdl = AutoModel.from_pretrained(EMB_MODEL).to(DEVICE)
mdl.eval()

def rule_vec(text):
    lf = text.lower()
    return np.array([sum(len(re.findall(rf'\b{re.escape(w)}\b', lf)) for w in keywords.get(cat, []))
                     for cat in ["violence", "sexual", "profanity", "alcohol_drugs", "scary"]], dtype=float)

# ===== Episode aggregates =====
from episodes_aggregates import episode_aggregates_for_scene

# ===== Load scene heads =====
if os.path.exists("heads.pkl"):
    with open("heads.pkl", "rb") as f:
        HEADS = pickle.load(f)
else:
    HEADS = None

# ===== Legal overrides (436-–§–ó) =====
OBSCENE_PATTERNS = [
    r"\b(–µ–±[–∞–æ]–Ω|–ø–∏–∑–¥|—Ö—É–π|–æ—Ö—É–µ–Ω–Ω|–±–ª—è–¥)\w*\b",
    r"\b(–Ω–∞—Ö—É–π|–ø–∏–¥–æ—Ä|—Å—É–∫–∞)\b",
]

DRUG_INSTRUCTIVE = [
    r"\b(–∫–∞–∫ (–ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å|—Å–¥–µ–ª–∞—Ç—å|–≤–∞—Ä–∏—Ç—å|–∑–∞–∫–ª–∞–¥–∫)|–¥–æ–∑–∏—Ä\w+|–∏–Ω—Å—Ç—Ä—É–∫—Ü\w+)\b",
]

DRUG_ROMANTICIZE = [
    r"\b(–∫–∞–π—Ñ|–æ—Ä–≥–∞–∑–º –æ—Ç|–∫–ª–∞—Å—Å–Ω–æ|–±–µ–∑ –ø–æ—Å–ª–µ–¥—Å—Ç–≤|–Ω–∏—á–µ–≥–æ —Å—Ç—Ä–∞—à–Ω–æ–≥–æ)\b",
]

NATURALISTIC_VIOLENCE = [
    r"\b(–Ω–∞—Ç—É—Ä–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫\w+|–∫—Ä—É–ø–Ω—ã–º –ø–ª–∞–Ω–æ–º|–≤ –¥–µ—Ç–∞–ª—è—Ö)\b",
    r"\b(–≤—Å–∫—Ä—ã–ª|–≤–Ω—É—Ç—Ä–µ–Ω–Ω–æ—Å—Ç|–∫–∏—à–∫|–∫—Ä–æ–≤–∏—â)\w*\b",
]

EXPLICIT_SEX = [
    r"\b(–ø–æ–ª–æ–≤–æ–π –∞–∫—Ç|–≤–≤–æ–¥–∏—Ç|—Ñ—Ä–∏–∫—Ü\w+|—ç—è–∫—É–ª—è—Ü\w+|–æ—Ä–∞–ª—å–Ω\w+)\b",
]

def any_match(text, patterns):
    low = text.lower()
    return any(re.search(p, low) for p in patterns)

def legal_overrides(scene_text):
    reasons = []
    if any_match(scene_text, OBSCENE_PATTERNS):
        reasons.append("–û–±—Å—Ü–µ–Ω–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞ (436‚Äë–§–ó)")
        return {"min_age": "18+", "reasons": reasons}
    if any_match(scene_text, DRUG_INSTRUCTIVE) or any_match(scene_text, DRUG_ROMANTICIZE):
        reasons.append("–ù–∞—Ä–∫–æ—Ç–∏–∫–∏: –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å/—Ä–æ–º–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (436‚Äë–§–ó)")
        return {"min_age": "18+", "reasons": reasons}
    if any_match(scene_text, NATURALISTIC_VIOLENCE):
        reasons.append("–ù–∞—Ç—É—Ä–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞—Å–∏–ª–∏–µ (436‚Äë–§–ó)")
        return {"min_age": "18+", "reasons": reasons}
    if any_match(scene_text, EXPLICIT_SEX):
        reasons.append("–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–µ–∫—Å—É–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (436‚Äë–§–ó)")
        return {"min_age": "18+", "reasons": reasons}
    return None

# ===== Thresholds and severity =====
THRESH = {"None": 0.2, "Mild": 0.4, "Moderate": 0.7}

def to_severity(p):
    if p < THRESH["None"]:
        return "None"
    if p < THRESH["Mild"]:
        return "Mild"
    if p < THRESH["Moderate"]:
        return "Moderate"
    return "Severe"

def analyze_scene(scene_text):
    rule_scores, episodes = rule_based_score(scene_text)
    ep_feats_vec = parse_ep_features(scene_text)
    epi = episode_aggregates_for_scene(scene_text)
    emb = scene_vector(scene_text, max_len=384, stride=320, batch_size=8, use_cache=True)
    rv = rule_vec(scene_text)
    
    if HEADS:
        x = np.hstack([emb, rv, ep_feats_vec, epi])
        model_probs = {cat: float(clf.predict_proba([x])[0, 1]) for cat, clf in HEADS.items()}
        epi_cat_max = {c: float(epi[i * 6 + 0]) for i, c in enumerate(["violence", "sexual", "profanity", "alcohol_drugs", "scary"])}
        final_probs = {cat: 0.55 * model_probs[cat] + 0.25 * rule_scores[cat] + 0.20 * epi_cat_max[cat]
                       for cat in ["violence", "sexual", "profanity", "alcohol_drugs", "scary"]}
    else:
        model_probs = {c: 0.0 for c in ["violence", "sexual", "profanity", "alcohol_drugs", "scary"]}
        epi_cat_max = {c: float(epi[i * 6 + 0]) for i, c in enumerate(["violence", "sexual", "profanity", "alcohol_drugs", "scary"])}
        final_probs = {cat: 0.80 * rule_scores[cat] + 0.20 * epi_cat_max[cat]
                       for cat in ["violence", "sexual", "profanity", "alcohol_drugs", "scary"]}
    
    severity = {cat: to_severity(p) for cat, p in final_probs.items()}
    per_class = {cat: {
        "rule_score": float(rule_scores[cat]),
        "model_proba": float(model_probs.get(cat, 0.0)),
        "episode_max": float(epi_cat_max[cat]),
        "final_proba": float(final_probs[cat]),
        "severity": severity[cat],
        "episodes": episodes[cat]
    } for cat in ["violence", "sexual", "profanity", "alcohol_drugs", "scary"]}
    
    return per_class

# ===== Age Rating =====
def age_from_scene(per_class):
    if per_class["profanity"]["severity"] in ["Moderate", "Severe"]:
        return "18+"
    if per_class["sexual"]["severity"] == "Severe":
        return "18+"
    if per_class["violence"]["severity"] == "Severe":
        return "18+"
    if per_class["violence"]["severity"] == "Moderate" or per_class["sexual"]["severity"] == "Moderate":
        return "16+"
    if per_class["alcohol_drugs"]["severity"] in ["Moderate", "Severe"]:
        return "16+"
    if per_class["scary"]["severity"] in ["Mild", "Moderate"]:
        return "12+"
    return "6+"

def aggregate_rating(scene_levels):
    order = ["0+", "6+", "12+", "16+", "18+"]
    worst = "0+"
    for r in scene_levels:
        if order.index(r) > order.index(worst):
            worst = r
    return worst

# ===== Main =====
def analyze_script(path, report_path="final_report.json"):
    text = read_script(path)
    text = normalize_headings(text)
    scenes = split_scenes(text)
    details, scene_levels = [], []
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ü–µ–Ω: {len(scenes)}")
    
    for i, s in enumerate(scenes, 1):
        meta = parse_header(s)
        per_class = analyze_scene(s)
        scene_rate = age_from_scene(per_class)
        
        override = legal_overrides(s)
        if override:
            order = ["0+", "6+", "12+", "16+", "18+"]
            if order.index(override["min_age"]) > order.index(scene_rate):
                scene_rate = override["min_age"]
        
        scene_levels.append(scene_rate)
        
        problems = []
        for cat, data in per_class.items():
            if data["severity"] in ["Moderate", "Severe"]:
                for ep in data["episodes"][:5]:
                    problems.append({
                        "category": cat,
                        "severity": data["severity"],
                        "snippet": ep["snippet"],
                        "offset": ep["offset"]
                    })
        
        if override:
            for r in override["reasons"]:
                problems.append({
                    "category": "legal",
                    "severity": "Severe",
                    "snippet": s[:240],
                    "offset": 0
                })
        
        details.append({
            "scene_index": i,
            **meta,
            "per_class": {k: {
                "rule_score": data["rule_score"],
                "model_proba": data["model_proba"],
                "episode_max": data["episode_max"],
                "final_proba": data["final_proba"],
                "severity": data["severity"],
                "episodes_count": len(data["episodes"])
            } for k, data in per_class.items()},
            "scene_rating": scene_rate,
            "problems": problems
        })
        
        print(f"–°—Ü–µ–Ω–∞ {i}: {scene_rate} | {meta.get('scene_no', '')} {meta.get('place_type', '')} {meta.get('location', '')} - {meta.get('tod', '')}")
    
    rating = aggregate_rating(scene_levels)
    
    def pct(cat):
        cnt = sum(1 for d in details if d["per_class"][cat]["severity"] in ["Mild", "Moderate", "Severe"])
        return round(100.0 * cnt / max(1, len(details)), 2)
    
    guide = {
        "violence": {"percentage_scenes": pct("violence"), "episodes_total": sum(d["per_class"]["violence"]["episodes_count"] for d in details)},
        "sexual": {"percentage_scenes": pct("sexual"), "episodes_total": sum(d["per_class"]["sexual"]["episodes_count"] for d in details)},
        "profanity": {"percentage_scenes": pct("profanity"), "episodes_total": sum(d["per_class"]["profanity"]["episodes_count"] for d in details)},
        "alcohol_drugs": {"percentage_scenes": pct("alcohol_drugs"), "episodes_total": sum(d["per_class"]["alcohol_drugs"]["episodes_count"] for d in details)},
        "scary": {"percentage_scenes": pct("scary"), "episodes_total": sum(d["per_class"]["scary"]["episodes_count"] for d in details)},
    }
    
    payload = {
        "rating": rating,
        "summary": {
            "count_scenes": len(scenes),
            "scene_ratings": {r: scene_levels.count(r) for r in ["6+", "12+", "16+", "18+"]}
        },
        "parents_guide": guide,
        "details": details
    }
    
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {rating}")
    print(f"üìÅ –°–æ—Ö—Ä–∞–Ω—ë–Ω –æ—Ç—á—ë—Ç: {report_path}")

if __name__ == "__main__":
    path = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ —Å—Ü–µ–Ω–∞—Ä–∏—é (.docx/.pdf): ").strip()
    if not os.path.exists(path):
        print("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    else:
        analyze_script(path)
